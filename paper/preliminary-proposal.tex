\documentclass[11pt]{article}
\input{tex-helpers/macros} 

\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Output font encoding for international characters
\usepackage{fouriernc} % Use the New Century Schoolbook font

\usepackage{lipsum} % Generates filler text.
\usepackage{natbib}
\bibliographystyle{abbrv}    % This is for having square brackets around references like [1].

\newcommand\todo[1]{\textcolor{red}{TODO: #1}}
\newcommand{\TITLE}{SHF: SMALL: SpecProg: Unifying Specifications and Code in Program Design}

%Project Description : Checklist

\begin{document}

\newpage \setcounter{page}{1}\thispagestyle{fancyplain}\headheight=24pt%
                        \fancyhead[C]{\bf \TITLE}
% page limit 2



% Related Work
\section{Related Work}

The cost of writing specifications is quite high and often the reason why the developers avoid writing specifications. However, if the specification is provided, it describes the behavior and intent of the concrete program. An alternative to this, specially specification written in natural language is using assertions \cite{Weintraub:1986}, a concept borrowed from proof techniques. This can be considered as very simplified, but an effective effort towards unifying specification and coding. Interestingly, to the best of our knowledge, a very early work \cite{Herman:1975} has first questioned the separation of specification and other programming activities.

Another visionary, Donald Knuth \cite{Knuth:1984} coined the idea of \emph{literate programming (LP)}, and demonstrated the possibility of unifying a document formatting language and a programming language. Surprisingly, there were not many follow-up works for the improvement of this programming paradigm, possibly due to the lack of techniques to implement this idea in the past. The most well-known of them are \cite{Lymperopoulos:2005, Kacofegitis:2003, Anderson:2001, Brown:2002, Ammers:2002, Roy:2006, Roy:2010, Palmer:2009}.  \cite{Lymperopoulos:2005} provided an application of literate programming in object-oriented code, to develop computational electromagnetics (CEM) library. In this work, literate programming environment FWEB for C++ language helps the users by generating human-readable documentation in TEX format. \cite{Kacofegitis:2003} provides a chunk model that allows unlimited code and document types, and a theme model to provide multiple views of a given system based on different descriptions of a program aimed at different groups. \cite{Anderson:2001} extends the literate programming tool using a document processor LYX to represent literate documents into tree representation. \cite{Brown:2002} describes the advantages of literate programming paradigm, and suggests that this paradigm should be used in the design phase of software development, as oppose to the practice of utilizing it in implementation and maintenance phases. VAMP \cite{Ammers:2002} is a tool for supporting literate programming through auto generate modules. Compared to its prior work WEB, VAMP is not restricted to a single programming language. \cite{Roy:2006} proposed P-Coder, a tool for creating a better description of the program using literate notation. \cite{Roy:2010} provides a tool LP/ Lisp, which supports literate programming for a language with incremental development and testing like LISP. Unlike previous LP tools, which are suitable only for compiled languages that naturally have a step between writing and executing the code, LP/Lisp supports the literate programming in running portions of the code. \cite{Palmer:2009} states about the challenges of literate programming for new programmers, since it consists of 3 different languages: the implementation language, documentation language and literate programming glue language. This work proposes Ginger, a language that considers literate programming language as core feature and proposes G-expression transformation for representing %naturally 
code as well as documentation and linear connections. In conclusion, existing literate programming research focuses on extending the original paradigm to support object-oriented programming and visualization tools. Now-a-days, research for improving literate programming with large-scale repositories and current natural language to programming language inference techniques  are needed.  One of the most important component in literate programming, the translation engine from natural to programming language were predefined by a subset of natural language grammar and limited dictionaries of natural language words.  So that, these prior works couldn't handle natural language descriptions which their grammar and words were not defined in the translation engine.  

Another closely related direction is the area of structural programming through refinement techniques. Niklaus Wirth \cite{Wirth:1971} first introduced the idea of gradually developing programs in a sequence of refinement steps to explore between preselection strategy and trial based stepwise construction. This notion has encouraged the programmers to concentrate more on designing \emph{abstract} programs \cite{Lekkos:1978, Morris:1973, Chand:1978}. This can, in result, speed up the process of proving program correctness, since proving correctness of \emph{abstract} program would suffice to prove the correctness of more detailed concrete program \cite{Hoare:1972, Milner:1971}. As modern software systems are inherently complex in nature and often remain in development stage in order to provide new and advanced features, incremental design has been embraced in both designing concrete programs \cite{Morrison:2011} and specifications of the program \cite{Nishida:1972}. Our approach is most similar to these works in a sense that we facilitate the specification-code duality that provides better abstraction and in addition expedites scalibility of verification. 

The benefits of refinement paradigm has rallied researchers towards introducing different techniques to transform abstract to concrete programs, e.g., \cite{Barstow:1978} proposes different alternative implementations to same abstract specification, \cite{Goncalves:2016} exploits domain knowledge towards program generation, \cite{Polikarpova:2016} introduces the mechanism and benefits of recursive program synthesis from polymorphic refinement types. Additionally, various program synthesis techniques \cite{WolframAlpha, Nazlia:2008, Maj:2014, Yin:2010, Butler:2017, Landhauber:2015, Desai:2016, Sharvari:2016, Raghothaman:2016, Yaghmazadeh:2017, Liu:2005, Knoll:2006} have been observed to answer open ended questions on combining logical specifications, natural languages to encourage an unifying programming interface \cite{Gulwani:2010}. Generally, these works proposed solution that suited for specific type of programming and specific programming language. One of most well-known application in this area is WolframAlpha \cite{WolframAlpha}. This work provided a computation engine that allows users to have the calculation output from well-known algorithms by their description in queries that commonly contain the name of algorithm and input arguments. \cite{Maj:2014} proposed a technique for generating assembly code from users’ natural language description for industrial robot actions. This work applied state-of-art NLP technique for semantic parsing and syntactic parsing to get the set of predicate-argument, which represent a sequence of tasks from the description, then mapping each predicate-argument to related action-object of the simulated word. The action-object is predefined and stored in a unified architecture. \cite{Yin:2010} designs a new language Quasi-Natural language that focuses on knowledge representation, which contains data structures and suitable operations. Thís language provides support for natural language which are expressed in certain lexical and syntactic rules. \cite{Desai:2017} provides a program synthesis approach for domain specific language (DSL)  by inferring a dictionary relation over pairs of English words and DSL terminal semi-automatically. \cite{Sharvari:2016} proposed another approach for program synthesis, which considers synonym problem and provides modules to handle each type of statements in a programming language. \cite{Landhauber:2015} focuses on the control structure of the programming language and provide an approach for using type dependency in natural language sentence to infer the its control structure. \cite{Nazlia:2008} proposes an heuristic approach to identify basic elements of object-oriented programming, including class name and variables/attributes. This work uses new heuristic rules manual from a training data set, then apply and evaluate the correctness of these rules in another set of natural language description corpus. SWIM \cite{Raghothaman:2016} uses data-driven approach to synthesize code snippet from users queries in two steps: extracting ranked APIs set for each query and, then, select structured call sequence for each API. The query to API model was learnt from \emph{clickthrough} data of Bing, while structured call sequence was learnt from Github code corpus. Pegasus \cite{Knoll:2006}, which is a first step towards implementating the ideas of naturalistic types \cite{Knoll:2011}, designs their approach in following pipeline: reading natural language, generating programming code and expressing natural language. One of most recent work \cite{Yaghmazadeh:2017} combines ideas of parsing technique from natural language and type-directed synthesis, program repair from programming language to generate SQL query from description. These highly advanced works can be leveraged to synthesize concrete programs and motivate programmers in regard to the practice of writing specifications.
% To be concluded, most of current works are focusing on generating code for a limited scope of programming tasks and languages. 

Additionally, Dunsmore \etal \cite{Dunsmore:1984} analyzes the programming efforts and state that ease of modification is related to the data communication and live variables. Therefore, replacing code by specification whenever possible can ease in terms of the programming effort required in software maintenance and evolution. Leino \etal \cite{Leino:2012} supports varying level of abstraction. The authors theorize that higher level abstraction can focus on the intent of the design, whereas the lower level abstraction can centralize on optimization and other detailed factors. SpecProg can provide similar advantages by supporting specification and code in unison.

As Knuth \cite{Knuth:1984} advocated that one of the primary goal behind bringing forth the specification is understanding the software itself. It enables multi-faceted advantages, e.g., facilitate maintaining legacy code, reuse of code etc. Naturally, the specification written in natural language is more human-readable compared to formal specifications. However, specification in natural language form does not suffice for verification purposes. If developers take a manual approach to transform between natural and programming languages, it will cost both in terms of effort and time. Besides, this can be often error prone, since the inference requires developers to have domain knowledge in both types of languages. To overcome these challenges, approaches to automatically derive natural language from programming language and vice versa have been designed.

To support better understanding of the source code by documentation, some approaches aimed to automatically generate some restricted type of natural language, e.g., generating pseudo code \cite{Oda:2015} and code comment \cite{Sridhara:2010}. \cite{Oda:2015} applied statistical machine translation (SMT) approach to generate pseudo-code from Python source code. This work proposed an improvement for original Phrase-To-Phrase translation commonly used in Natural Language Processing (NLP), by providing a Tree-To-String translation approach. They built a parallel corpus at method level, where, the source language reflects Abstract-Syntax-Tree, and target language reflects pseudo code of the same method. This work limited the generation process to statement-by-statement, meaning that the pseudo-code was generated for each statement in the source code. This process is insufficient, however, for generating pseudo-code in large software project environments that contains more than thousand lines of code. \cite{Sridhara:2010} proposed an approach to extract the comments from Java methods that summarizes Java methods’ content. This work also focus at method level, by the extracting a set of important central lines of code (s-unit). Then it generates the summary for lines in s-unit using a \emph{software word usage model} constructed for each method.

The inference from natural language to programming language seems to be tackled more compared to the other direction of reference. There are researches on analyzing Natural Language grammar versus Programming Language grammars and researches on approaches for the inference of Programming Language from Natural Language.

To analyze natural language and programming panguage grammars, one of the starting point was made in 1977 by L. Miller \cite{Miller:1977}. This work proposed \emph{Natural Language Programming}, which analyzes natural language programs to have semantic representation of 1000 unique-words. \cite{Osvaldo:2013} analyzed the differences between programming language grammars, which were built by Context-Free-Grammar versus natural language grammar. The experiment shows that using natural language grammar can be used as an alternative for humans to learn the fundamental of programming, which is more attractive and promising. \cite{Knoll:2011} analyses characteristics of natural language by the mapping programming language.  This work proposed the idea of naturalistic types, which contains 4 elements: concept, properties, quantities and conditions. In game programming development, \cite{Judith:2015} argued that entire natural language will not be a viable option for coding. Based on the experiments, the authors suggested some design rules for making use of natural language, among these are: constrain the programming language to minimize syntax error and the natural language used should be consistent with typical everyday use and avoid unnatural syntax or phrasing. Contary to these approaches \cite{Gopstein:2017} analyzed source codes and provided experiments to identify which patterns in source code can lead to misunderstanding in software development.
 
There have been some other ongoing research \cite{Knoll:2006, Maragoudakis:2008, Ballard:1979, Bruckman:1999, Chong:2004} on motivating towards \emph{naturalistic} programming languages. Efforts \cite{Arnold:2010, Begel:2006} have been made to address the one of the main issues in naturalistic programming, which is dealing with ambiguity. Clark \etal debates the trade-off between \emph{naturalness} and \emph{predictability} and proposes a synthesis of embedded deterministic core in naturalast controlled languages. However these efforts only tries to bridge the gap between natural and programming languages, contrary to our objective of unifying the specification and code as a single programming paradigm. Swartout \etal advocates intertwining of specification and implementation as oppose to completing the specification process in advance. A closely related work is \cite{Qiu:2017}, that takes a program template together with rich functional specification and produces concrete program with proof artifacts. While these works can be leveraged towards specification-code transformation and refinement, these are only partial focus in the path of unification of specification-code.




%\noindent
%{\bf Specification Inference.}
%There has been significant effort \cite{Rajan:2015, Fraser:2011,Nguyen:2011, Dallmeier:2010, Renieris:2004, Rinard:1997, Ruthruff:2006, Aleen:2009} involved to solve the lack of specification problem. Broadly, the approaches to infer specifications to address this issue can be classified into two different categories: Usage-based approach and Implementation-based approach. 
%
%Typically, state-of-art usage-based approaches \cite{Ramanathan:2007, Nguyen:2014, Michail:2000, Nguyen:2009, Livshits:2005, Li:2005, Zhong:2009} mines different call sites to analyze the usage of a method and collect set of predicates along each distinct path. Some of these technique depends on frequent itemset mining, subsequence mining on data and control flow results, where as others concentrate on more light-weight static analysis. In contrast, the implementation based approaches rely on the implementation of the method for which the specification is to be inferred. There are static analysis based approaches e.g., \cite{Cousot:2013} uses abstract interpretation, \cite{Buse:2008} uses symbolic execution to infer specifications. Other techniques perform dynamic analysis \cite{Ammons:2002, Beschastnikh:2011, Ernst:1999, Cousot:2011, Gabel:2008} to synthesize specifications. 
%
%The type of specification that is generally inferred from these approaches are of two types: behavioral specification and temporal specification. The behavioral specification signifies the behavior of methods (using pre- and
%post-conditions) and modules (e.g., using invariants in classes
%and interfaces). Some efforts \cite{Nguyen:2014, Rajan:2015, Wei:2011} have been seen to infer these behavioral specifications. Contrarily, the temporal specifications refers to the order of method calls/API calls that needs to be satisfied. Some approaches detect this as a pair of method invocation \cite{Wasylkowski:2007, Nguyen:2009, Livshits:2005}, or, sequence of method calls \cite{Thummalapenta:2009}, where as, some approaches direct their focus on association of API entities \cite{Li:2005}. 
%
%Formal specification is an integral part of software reliability and lack of specification has directed researchers in synthesizing and inferring specifications in an automated manner. While these approaches tries to unburden the developers from cost-benefit trade-off of writing specifications, the question of maintaining two different artifacts and checking the conformity between these two at a regular basis can considerably add in to the cost. In contrast, unification of specifying and programming together can reduce the maintenance cost and provide safety-critical, reliable software. \\

%\noindent
%{\bf Natural Language Processing.}
%At times developers provide documentation in the format of natural language for APIs. Specification written in natural language is more human-readable and it can make maintaining legacy code easier for this reason. Developers may feel that some piece of code can be better explained through natural language. Our unifying programming methodology allows specification to be written in natural language, in addition to behavioral and temporal specifications. 
%
%However, it is often necessary to find a sound replaceble code of such specifications to verify the correctness of the software. There has been some related work (add reference) that can construct program/code fragments/expressions based on natural language. These techniques involve NLP parse trees (stanford ref?), combinatory categorical grammars (ref?), probabilistic context free grammars (anycode ref?) in order to produce automatically generated programs. Most of these works have been applied for program synthesis purpose in order to facilitate effortless programming. Contrary to that, part of this work transforms natural language documentation to formal specification/code fragments to provide specification-code duality.   \\



% add bibliography at start of a new page
\cleardoublepage

% add the bibliography
\renewcommand{\bibname}{\centerline{BIBLIOGRAPHY}}
\interlinepenalty=300
\bibliography{refs}

\end{document}
