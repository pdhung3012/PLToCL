
\documentclass[english]{lipics-v2016}
\usepackage{url}
\title{Related Work}
\usepackage{notoccite}
\usepackage[numbers,sort&compress]{natbib}
\begin{document}

\section{Related Work}

In Software Engineering, documentation on source code provides an important role in Software Development. Documentation help users understand the behavior of the program built by that source code. In the other hand, documentation can be provided as an input for developers to develop program. Documentation, which is described in the form of natural language, reflects the same intent of the behavior of the program in programming languages. Inferring natural language from programming language and vice versa, which are usually done manually by software developers, are effort consuming and error prone, since the inference require developers to have knowledge from writing both type of languages. To overcome this challenge, approaches for automatically deriving natural language and vice versa have been designed.

To support developers for better understanding source code by documentation in some restricted type of natural language, there are researches on generating pseudo code \cite{p2} and code comment \cite{p24}.  \cite{p2} applied Statistical Machine Translation (SMT) approach to generate pseudo-code from Python source code. This work proposed an improvement for original Phrase-To-Phrase translation commonly used in Natural Language Processing (NLP), by providing a Tree-To-String translation approach. They built a parallel corpus at method levels, with the source language reflects Abstract-Syntax-Tree, and target language reflects pseudo code of the same method. This work limited the generation process statement-by-statement, means the pseudo-code was generated for each statement in the source code. This limitation caused insufficient for generated pseudo-code in large software project environments contains more than thousands line of code. \cite{p24} proposed an approach for extract the comments from Java Methods that summarize Java Methods’ content. This work also generate at method level, by the extraction of  s-unit, which is a set of important central lines of code, then generate the summary for lines in s-unit using a Software Word Usage Model constructed for each method.

The inference from Natural language to Programming language seems to be more tackle compare to the other direction of reference, since this problem needs to deal with Natural Language Grammar analysis. There are researches on analyzing Natural Language grammar versus Programming Language grammars and researches on approaches for the inference of Programming Language from Natural Language..

For analyzing Natural Language and Programming Language grammar, one of the starting point was made in 1977 by \cite{p1}. This work proposed  a project name Natural Language Programming,which analyzing natural language programs to have semantic representation of 1000 unique-words. \cite{p5} provided experiments to identifies which patterns in source code can lead to misunderstanding in software development. \cite{p12} analyzed the differences between programming language grammars, which were built by Context-Free-Grammar versus Natural Language grammar. The experiment shows that using Natural Language grammar can be used as an alternative for human to learn the fundamental of programming, which is more attractive and promising. \cite{p25} analyses characteristics of natural language by the mapping programming language. This work proposed the idea of naturalistic types, which contain 4 elements: concept, properties, quantities and conditions. In game programming development, \cite{p16} argued that entire natural language will not be a viable option for coding. Based on the experiment. \cite{p16} suggested some design rules for making use of natural language, two of them are constrain the programming language to minimize syntax error and the natural language used should be consistent with typical everyday use and avoid unnatural syntax or phrasing.

There are researches on approaches for synthesizing programming language in \cite{p4} \cite{p11} \cite{p13}  \cite{p15} \cite{p17} \cite{p18} \cite{p19} \cite{p23} \cite{p26} and  \cite{p3}. General, these work proposed solution that suited for specific types of programming and specific programming language. One of most well-known application in this area is WolframAlpha \cite{p4}. This work provided a Computation Engine that allow users to have the calculation output from well-known algorithms by their description in queries, commonly contain the name of algorithm and input arguments. \cite{p11} proposed a technique for generating assembly code from users’ natural language description for industrial robot actions. This work applied state-of-the-art NLP technique for semantic parsing and syntactic parsing to get the set of predicate-argument  which represent a sequence of tasks from the description, then mapping each predicate-argument to related action-object of the simulated word. The action-object is predefined and stored in a unified architecture. \cite{p13} designs a new language Quasi-Natural language that focus on knowledge representation, which contains data structures and suitable operations. This language provides support for natural language which are in certain lexical and syntactic rules. \cite{p14} provides a program synthesis approach for Domain Specific Language (DSL)  by inferring a dictionary relation over pairs of English words and DSL terminal semi-automatically. \cite{p15} proposed another approach for program synthesis, which consider synonyms problem and provide modules to handle each type of statements in programming language. \cite{p17} focuses on the Control structure of the programming language and provide an approach for using type dependency in Natural language sentence to infer the its control structure. \cite{p18} proposes an heuristic approach to identify basic elements of Object-Oriented Programming, including class name and variables/ attributes. This work uses format new heuristics rules manual from a training data set, then apply and evaluation the correctness of these rules in another set of natural language description corpus. SWIM \cite{p19} uses data-driven approach to synthesize code snippet from users queries by 2 steps: extracting Ranked APIs set for each query and selected structured call sequence for each API. The Query to API model was learnt from Clickthrough data of Bing while Structured call sequence was learnt from Github Code corpus. Pegasus \cite{p26}, which is a first step implementation of ideas of naturalistic types in  \cite{p25} designs their approach by 3 processes: reading natural language, generating programming code and expressing natural language. One of most recent work, \cite{p3} combines ideas of parsing technique from Natural language and type-directed synthesis and program repair from Programming language to generate SQL query from description. To be concluded, most of current works are focusing on generating code for a limited scope of programming tasks and languages.





The advantages of automatically inference leads to researches on how to fulfill the gap between natural language and programming language by an unified language structure. The most famous programming paradigm supported this idea is literate programming, which was invented by Donald Knuth \cite{p27}. Surprisingly, there were not many publications for the improvement from this programming paradigm, possibly due to the lack of techniques to implement this idea in the past. The most well-known of them are \cite{p6} \cite{p7} \cite{p8} \cite{p9} \cite{p10} \cite{p20} \cite{p21} \cite{p22}.  \cite{p6} provided an application of literate programming in object-oriented code, to support develop computational Electromagnetics (CEM) library. In this work, literate programming environment FWEB for C++ language help the users by generating human-readable documentation in TEX format. \cite{p7} provides a chunk model which allow unlimited code and document types, and a theme model which provide multiple views of a given system based on different descriptions of a program aimed at different group.  \cite{p8} extends the literate programming tool to support users, using a document processor LYX to represent literate documents by tree representation. \cite{p9} describes about advantages of literate programming paradigm, and the this paradigm should be used in the design phase of software development, besides implementation and maintenance phases as before. VAMP \cite{p10} is a tool for support literate programming by auto generate modules. Compared to its prior work WEB, VAMP is not restricted to a single programming language. \cite{p20} proposed P-Coder, a tool for creating a better description of the program using literate notation. \cite{p21} provides a tool LP/ Lisp, which support literate programming for a language with incremental development and testing like LISP. Unlike previous LP tools which are suitable only for compiled languages that naturally have a step between writing and executing the code, LP/Lisp supports the Literate Programming in running portions of the code. \cite{p22} states about challenge of literate programming for new programmer, since it consists of 3 different languages: the implementation language, documentation language and literate programming glue language.This work propose Ginger, a language that considers literate programming language as core feature and propose G-expression transformation for represent naturally code as well as documentation and linear connections. In concluded, existing literate programming researches focuses on extending the original paradigm to support Object-Oriented programming and visualization tools. In general, these existing works focus on supporting literate programming in different programming languages and better interaction with developers when using this paradigm. One of the most important component in literate programming, the translation engine from natural to programming language were predefined by a subset of natural language grammar and limited dictionaries of natural language words.  So that, these prior works couldn't handle natural language descriptions which their grammar and words were not defined in the translation engine.  





\bibliographystyle{unsrtnat}
\bibliography{combineRelatedWork}


\end{document}
